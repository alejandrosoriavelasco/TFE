{"cells":[{"cell_type":"code","source":["import datetime, warnings, scipy \nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport glob\nfrom itertools import chain\n\nfrom pyspark.sql import functions as fn\nfrom pyspark.sql.functions import col as col\nfrom pyspark.sql.types import StringType\nfrom pyspark.sql import Window\nfrom pyspark.sql import Row \nfrom pyspark.sql.types import IntegerType, TimestampType, DateType\nfrom pyspark.sql.functions  import to_timestamp\nfrom pyspark.sql.functions import create_map, lit, avg, count\nfrom pyspark.sql.functions import year, month, dayofweek, hour, minute, second\nfrom pyspark.ml.feature import StringIndexer"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["def clean_dataset(df):\n    \n    df = df.drop('OP_CARRIER_FL_NUM', 'DEP_TIME', 'CRS_ARR_TIME', 'ARR_TIME','TAXI_OUT', 'WHEELS_OFF','WHEELS_ON','TAXI_IN', \n                           'CANCELLED', 'CANCELLATION_CODE','DIVERTED','AIR_TIME','CARRIER_DELAY','WEATHER_DELAY',\n                           'NAS_DELAY', 'SECURITY_DELAY','LATE_AIRCRAFT_DELAY','CRS_ELAPSED_TIME','ACTUAL_ELAPSED_TIME',\n                           'Unnamed: 27')\n\n    \n    mapping = dict(zip(['FL_DATE', 'OP_CARRIER', 'DEST', 'CRS_DEP_TIME', 'DEP_DELAY', 'ARR_DELAY'], ['DATE', 'AIRLINE', 'DESTIN', 'SCHED_DEPARTURE', 'DEPARTURE_DELAY', 'ARRIVAL_DELAY']))\n    df = df.select([col(c).alias(mapping.get(c, c)) for c in df.columns])\n    \n\n    df = df.dropna(subset=df.columns)\n    \n    return df\n\ndef time_format(df):\n  df = df.withColumn('DATE_TIME', fn.format_string(\"%04d\", col(\"SCHED_DEPARTURE\")))\\\n    .withColumn(\n        'DATE_TIME',\n        fn.concat_ws(\n            \":\",\n            fn.array(\n                [\n                    fn.substring(\n                        'DATE_TIME',\n                        1,\n                        2\n                    ),\n                    fn.substring(\n                        'DATE_TIME',\n                        3,\n                        2\n                    ),\n                    fn.lit(\"00\")\n                ]\n            )\n        )\n    )\\\n  .withColumn('DATE_TIME', to_timestamp(fn.concat(col('DATE').cast('date'), col('DATE_TIME')), \"yyyy-MM-ddHH:mm:ss\"))\n  df = df.drop('DATE')\n  return df\n    \n\ndef airline_name(df):\n  mapping = dict(zip(airlines.select('CODE').rdd.flatMap(lambda x: x).collect(), airlines.select('AIRLINE').rdd.flatMap(lambda x: x).collect()))\n  mapping_expr = create_map([lit(x) for x in chain(*mapping.items())])\n  \n  df = df.withColumn('AIRLINE_NAME', mapping_expr[col(\"AIRLINE\")])\n    \n  return df\n\ndef geo (df):\n  latitud_mapping = dict(zip(airports.select('IATA_CODE').rdd.flatMap(lambda x: x).collect(), airports.select('LATITUDE').rdd.flatMap(lambda x: x).collect()))\n  latitud_mapping_expr = create_map([lit(x) for x in chain(*latitud_mapping.items())])\n  \n  longitude_mapping = dict(zip(airports.select('IATA_CODE').rdd.flatMap(lambda x: x).collect(), airports.select('LONGITUDE').rdd.flatMap(lambda x: x).collect()))\n  longitude_mapping_expr = create_map([lit(x) for x in chain(*longitude_mapping.items())])\n  \n  df = df.withColumn('OR_LATITUDE', latitud_mapping_expr[col(\"ORIGIN\")])\n  df = df.withColumn('OR_LONGITUDE', longitude_mapping_expr[col(\"ORIGIN\")])\n  \n  df = df.withColumn('DEST_LATITUDE', latitud_mapping_expr[col(\"DESTIN\")])\n  df = df.withColumn('DEST_LONGITUDE', longitude_mapping_expr[col(\"DESTIN\")])\n    \n  return df\n\n\ndef outlier (df):\n  #Outliers \n  df = df.filter((col(\"ARRIVAL_DELAY\") > -30))\n  return df\n\n\ndef get_stats(group):\n    return {'min': group.min(), 'max': group.max(),\n            'count': group.count(), 'mean': group.mean()}"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["airports = spark.read.format('csv').options(header='true', inferSchema='true', delimiter=',') \\\n.load('/FileStore/tables/dict/airports.csv')\nairlines = spark.read.format('csv').options(header='true', inferSchema='true', delimiter=';') \\\n.load('/FileStore/tables/dict/allairlines.csv')"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["%fs rm -r /my.csv"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# Cleaning and Preprocesing\npaths= [\"/FileStore/tables/data/2014.csv\", \"/FileStore/tables/data/2015.csv\",\"/FileStore/tables/data/2016.csv\",\"/FileStore/tables/data/2017.csv\"]\ndata = spark.read.format('csv').options(header='true', inferSchema='true', delimiter=',') \\\n.load(paths)\ndata.cache()\n# Cleaning\ndata = clean_dataset(data)\n# Datetime Transform\ndata = time_format(data)\n# Outliers\ndata = outlier(data)\n#Airlines names\ndata = airline_name(data)\n#Geo\ndata = geo(data)\ndata.cache()\n# Saving\n# data.coalesce(1).write.format('com.databricks.spark.csv').save('clean_data.csv',header = 'true')"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["## American Airlines (AA)\ndf_AA=data.filter(col('AIRLINE')=='AA')\ndf_AA=df_AA.drop('AIRLINE','AIRLINE_NAME','OR_LATITUDE', 'OR_LONGITUDE', 'DEST_LATITUDE', \n                 'DEST_LONGITUDE','DEPARTURE_DELAY','SCHED_DEPARTURE')\n# Timestamp to column\ndf_AA = df_AA.withColumn('MONTH',month(df_AA.DATE_TIME))\ndf_AA = df_AA.withColumn('HOUR',hour(df_AA.DATE_TIME))\ndf_AA = df_AA.withColumn('YEAR',year(df_AA.DATE_TIME))\ndf_AA = df_AA.withColumn('DAYOFWEEK',dayofweek(df_AA.DATE_TIME))\ndf_AA = df_AA.withColumn('SCHEDULED_DEPARTURE', hour(df_AA.DATE_TIME)*3600+minute(df_AA.DATE_TIME)*60+second(df_AA.DATE_TIME))\n# StringIndexer\nindexer = StringIndexer(inputCol='ORIGIN', outputCol=\"ORIGIN_LABEL\") \ndf_AA = indexer.fit(df_AA).transform(df_AA) \nindexer = StringIndexer(inputCol='DESTIN', outputCol=\"DESTIN_LABEL\") \ndf_AA = indexer.fit(df_AA).transform(df_AA)\n# Classes\ndf_AA = df_AA.withColumn('DELAY_LEVEL',\n    fn.when((col(\"ARRIVAL_DELAY\") < 15), 0)\\\n    .when((fn.col(\"ARRIVAL_DELAY\") < 50), 1)\\\n#     .when((fn.col(\"ARRIVAL_DELAY\") < 120), 2)\\\n    .otherwise(2))\n\ndf_AA.cache()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["display(df_AA) #year avg"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["display(df_AA) # year count"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["display(df_AA) #month avg"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["display(df_AA) #dayof week avg"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["%fs rm -r /FileStore/tables/clean/clean_data_2018/"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# Test preparing - Data 2018\npaths= [\"/FileStore/tables/data/2018.csv\"]\ndata_2018 = spark.read.format('csv').options(header='true', inferSchema='true', delimiter=',') \\\n.load(paths)\ndata_2018.cache()\n# Cleaning\ndata_2018 = clean_dataset(data_2018)\n# Datetime Transform\ndata_2018 = time_format(data_2018)\n# Outliers\ndata_2018 = outlier(data_2018)\n#Airlines names\ndata_2018 = airline_name(data_2018)\n#Geo\ndata_2018 = geo(data_2018)\n# Saving\n# data_2018.write.format(\"com.databricks.spark.csv\").save(\"/FileStore/tables/clean/clean_data_2018\")\n\n## American Airlines (AA)\ndf_AA_2018=data_2018.filter(col('AIRLINE')=='AA')\ndf_AA_2018=df_AA_2018.drop('AIRLINE','AIRLINE_NAME','OR_LATITUDE', 'OR_LONGITUDE', 'DEST_LATITUDE', \n                 'DEST_LONGITUDE','DEPARTURE_DELAY','SCHED_DEPARTURE')\n# Timestamp to column\ndf_AA_2018 = df_AA_2018.withColumn('MONTH',month(df_AA_2018.DATE_TIME))\ndf_AA_2018 = df_AA_2018.withColumn('HOUR',hour(df_AA_2018.DATE_TIME))\ndf_AA_2018 = df_AA_2018.withColumn('YEAR',year(df_AA_2018.DATE_TIME))\ndf_AA_2018 = df_AA_2018.withColumn('DAYOFWEEK',dayofweek(df_AA_2018.DATE_TIME))\ndf_AA_2018 = df_AA_2018.withColumn('SCHEDULED_DEPARTURE', hour(df_AA_2018.DATE_TIME)*3600\n                                   +minute(df_AA_2018.DATE_TIME)*60+second(df_AA_2018.DATE_TIME))\n# Labeling\nindexer = StringIndexer(inputCol='ORIGIN', outputCol=\"ORIGIN_LABEL\") \ndf_AA_2018 = indexer.fit(df_AA_2018).transform(df_AA_2018) \nindexer = StringIndexer(inputCol='DESTIN', outputCol=\"DESTIN_LABEL\") \ndf_AA_2018 = indexer.fit(df_AA_2018).transform(df_AA_2018)\n# Classes\ndf_AA_2018 = df_AA_2018.withColumn('DELAY_LEVEL',\n    fn.when((col(\"ARRIVAL_DELAY\") < 15), 0)\\\n    .when((fn.col(\"ARRIVAL_DELAY\") < 50), 1)\\\n#     .when((fn.col(\"ARRIVAL_DELAY\") < 120), 2)\\\n    .otherwise(2))"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["display(df_AA_2018) #month avg 2018"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["display(df_AA_2018) #dayofweek avg 2018"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["print((df_AA.count(), len(df_AA.columns)))\nprint((df_AA_2018.count(), len(df_AA_2018.columns)))"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["print('Conjunto de Train')\ndf_AA.groupBy(df_AA['DELAY_LEVEL'].cast(IntegerType()).alias('DELAY_LEVEL')).count().show(10)\nprint('Conjunto de Test')\ndf_AA_2018.groupBy(df_AA_2018['DELAY_LEVEL'].cast(IntegerType()).alias('DELAY_LEVEL')).count().show(10)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["#Undersampling\nhighdelay = df_AA.filter(fn.col('DELAY_LEVEL')=='2.0')\nmediumdelay = df_AA.filter(fn.col('DELAY_LEVEL')=='1.0')\nontime = df_AA.filter(fn.col('DELAY_LEVEL')=='0.0')\nsampleRatio = float(mediumdelay.count()) / float(ontime.count())\nSampleDf = ontime.sample(False, sampleRatio)\ndf_AA2 = mediumdelay.unionAll(SampleDf)\ndf_AA_under = df_AA2.unionAll(highdelay)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["highdelay = df_AA_2018.filter(fn.col('DELAY_LEVEL')=='2.0')\nmediumdelay = df_AA_2018.filter(fn.col('DELAY_LEVEL')=='1.0')\nontime = df_AA_2018.filter(fn.col('DELAY_LEVEL')=='0.0')\nsampleRatio = float(mediumdelay.count()) / float(df_AA_2018.count())\nSampleDf = ontime.sample(False, sampleRatio)\ndf_AA_2018_2 = mediumdelay.unionAll(SampleDf)\ndf_AA_2018_under = df_AA_2018_2.unionAll(highdelay)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["print('Conjunto de Train')\ndf_AA_under.groupBy(df_AA['DELAY_LEVEL'].cast(IntegerType()).alias('DELAY_LEVEL')).count().show(10)\nprint('Conjunto de Test')\ndf_AA_2018_under.groupBy(df_AA_2018['DELAY_LEVEL'].cast(IntegerType()).alias('DELAY_LEVEL')).count().show(10)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.classification import FMClassifier\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.mllib.evaluation import MulticlassMetrics\n\ndf=df_AA_under.drop('ORIGIN','DESTIN','DATE_TIME','ARRIVAL_DELAY','DISTANCE','YEAR')\ndf2018=df_AA_2018.drop('ORIGIN','DESTIN','DATE_TIME','ARRIVAL_DELAY','DISTANCE','YEAR')\n\n# Realizamos un assebler para unir todas las columnas predictoras del dataset en un vector de features\nvecAssembler = VectorAssembler(inputCols=['MONTH', 'HOUR', \n                                          'SCHEDULED_DEPARTURE','DAYOFWEEK','ORIGIN_LABEL','DESTIN_LABEL'], outputCol=\"features\")\nDfAssembled = vecAssembler.transform(df)\nDfAssembled2018 = vecAssembler.transform(df2018)\nDfAssembled.show(3)\n\n# Creamos los DataFrame de entrenamiento y test\n#splitado = DfAssembled.randomSplit([0.7, 0.3], 124)\ndfTraining = DfAssembled\ndfTest = DfAssembled2018\n\n# Definimos el algoritmo de RandomForest y creamos el modelo \nrf = RandomForestClassifier(labelCol=\"DELAY_LEVEL\", numTrees=900, maxDepth=4, seed=42, maxBins=111)\nrfModel = rf.fit(dfTraining)\n\n# Aplicamos el modelo sobre el conjunto de test para obtener las prediciones \nprediccionDfTest = rfModel.transform(dfTest)\nprediccionDfTest.select(prediccionDfTest.features, prediccionDfTest.probability, \\\n                        prediccionDfTest.DELAY_LEVEL, prediccionDfTest.prediction).show(5, truncate=False)\n\n# Aplicamos el evaluador multiclase par ver la eficiencia del modelo\nevaluator = MulticlassClassificationEvaluator(labelCol=\"DELAY_LEVEL\")\nprint('Precision: {0}, Recall: {1}, F1: {2}'.format(evaluator.evaluate(prediccionDfTest, {evaluator.metricName: \"weightedPrecision\"}), \\\n                                           evaluator.evaluate(prediccionDfTest, {evaluator.metricName: \"weightedRecall\"}), \\\n                                           evaluator.evaluate(prediccionDfTest, {evaluator.metricName: \"f1\"})))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+----+---------+-------------------+------------+------------+-----------+--------------------+\nMONTH|HOUR|DAYOFWEEK|SCHEDULED_DEPARTURE|ORIGIN_LABEL|DESTIN_LABEL|DELAY_LEVEL|            features|\n+-----+----+---------+-------------------+------------+------------+-----------+--------------------+\n    1|  21|        4|              78900|         5.0|         3.0|          1|[1.0,21.0,78900.0...|\n    1|   5|        4|              21300|         5.0|         3.0|          1|[1.0,5.0,21300.0,...|\n    1|  23|        4|              84300|         5.0|        10.0|          1|[1.0,23.0,84300.0...|\n+-----+----+---------+-------------------+------------+------------+-----------+--------------------+\nonly showing top 3 rows\n\n+-------------------------------+------------------------------------------------------------+-----------+----------+\nfeatures                       |probability                                                 |DELAY_LEVEL|prediction|\n+-------------------------------+------------------------------------------------------------+-----------+----------+\n[1.0,11.0,42900.0,2.0,8.0,0.0] |[0.5107220125508114,0.3296128616519689,0.15966512579721975] |0          |0.0       |\n[1.0,7.0,25500.0,2.0,0.0,8.0]  |[0.5124305038882764,0.33810293362242,0.14946656248930362]   |0          |0.0       |\n[1.0,11.0,42480.0,2.0,17.0,5.0]|[0.4301022544787139,0.37853075626033544,0.19136698926095055]|0          |0.0       |\n[1.0,8.0,30300.0,2.0,5.0,17.0] |[0.44269068279908996,0.3713168766309492,0.18599244056996092]|0          |0.0       |\n[1.0,11.0,42900.0,2.0,8.0,1.0] |[0.5148845305692568,0.3285760605647671,0.15653940886597612] |0          |0.0       |\n+-------------------------------+------------------------------------------------------------+-----------+----------+\nonly showing top 5 rows\n\nPrecision: 0.6894586924093905, Recall: 0.4909721694649142, F1: 0.5422951423341746\n</div>"]}}],"execution_count":20},{"cell_type":"code","source":["rfModel.featureImportances"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[18]: SparseVector(6, {0: 0.1009, 1: 0.2961, 2: 0.4461, 3: 0.0, 4: 0.1174, 5: 0.0395})</div>"]}}],"execution_count":21},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix\ny_true = prediccionDfTest.select('DELAY_LEVEL')\ny_true = y_true.toPandas()\n\ny_pred = prediccionDfTest.select('prediction')\ny_pred = y_pred.toPandas()\n\ncnf_matrix = confusion_matrix(y_true, y_pred)\nprint(cnf_matrix)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[[359480 328484      0]\n [ 42617  67056      0]\n [ 22531  48590      0]]\n</div>"]}}],"execution_count":22},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":23}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.7.4","nbconvert_exporter":"python","file_extension":".py"},"name":"Producto PySpark","notebookId":1984798620956560},"nbformat":4,"nbformat_minor":0}
